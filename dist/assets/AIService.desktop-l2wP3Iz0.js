import{i as u}from"./core-CLjdq0uY.js";import{A as w}from"./index-BaRj8WfR.js";import{C as E,G as C,O as $}from"./index-BaRj8WfR.js";const b={"llama3.1:8b":{id:"llama3.1:8b",name:"Llama 3.1 8B",size:"4.7 GB",speed:"Fast",description:"Best for large files (128K context)"},"qwen2.5-coder:7b":{id:"qwen2.5-coder:7b",name:"Qwen2.5 Coder 7B",size:"4.7 GB",speed:"Medium",description:"High quality code generation"},"qwen2.5-coder:3b":{id:"qwen2.5-coder:3b",name:"Qwen2.5 Coder 3B",size:"2 GB",speed:"Fast",description:"Balanced performance"},"qwen2.5-coder:1.5b":{id:"qwen2.5-coder:1.5b",name:"Qwen2.5 Coder 1.5B",size:"1 GB",speed:"Very Fast",description:"Fast, small files only"},"deepseek-r1:8b":{id:"deepseek-r1:8b",name:"DeepSeek R1 8B",size:"4.9 GB",speed:"Medium",description:"Reasoning model (not for large files)"},"codellama:7b":{id:"codellama:7b",name:"CodeLlama 7B",size:"3.8 GB",speed:"Medium",description:"Meta's code model"},"llama3.2:3b":{id:"llama3.2:3b",name:"Llama 3.2 3B",size:"2 GB",speed:"Fast",description:"General purpose"}};class y{constructor(){this.ollamaAvailable=null,this.availableModels=[]}async checkOllamaStatus(){try{const a=await u("check_ollama_status");return this.ollamaAvailable=a.available,this.availableModels=a.models||[],a}catch(a){return console.error("Failed to check Ollama status:",a),{available:!1,error:a.toString(),models:[]}}}async isModelAvailable(a){try{return await u("check_model_available",{model:a})}catch(e){return console.error(`Failed to check model ${a}:`,e),!1}}async pullModel(a,e){try{e&&e({progress:0,text:`Downloading ${a}...`,timeElapsed:0});const t=await u("pull_ollama_model",{model:a});return e&&e({progress:100,text:"Model downloaded successfully",timeElapsed:0}),t}catch(t){throw new Error(`Failed to pull model: ${t}`)}}async getDownloadedModels(){return(await this.checkOllamaStatus()).models||[]}buildFixPrompt(a,e){var i;const t=((i=e.allErrors)==null?void 0:i.map(o=>`Line ${o.line}: ${o.message}`).join(`
`))||e.message;return`You are a ${e.type} syntax error fixer. Your task is to fix ONLY the syntax errors in the provided content.

Errors found:
${t}

Content to fix:
${a}

Instructions:
1. Fix ONLY the syntax errors listed above
2. Preserve all data and structure
3. Do not add explanations or comments
4. Return ONLY the complete corrected ${e.type}
5. Ensure the output is valid ${e.type}

Fixed ${e.type}:`}async fixWithOllama(a,e,t,i){if(!await this.isModelAvailable(t))throw new Error(`Model ${t} not found. Please download it first from AI Settings.`);try{i&&i({progress:50,text:"Processing with Ollama...",timeElapsed:0});const r=await u("fix_with_ollama",{content:a,errorDetails:JSON.stringify(e),model:t});return i&&i({progress:100,text:"Fixed successfully",timeElapsed:0}),r.trim()}catch(r){throw new Error(`Ollama fix failed: ${r}`)}}extractContent(a,e){const t=a.trim();if(e==="JSON"){const i=t.match(/[{\[]/);if(!i)return t;const o=i.index,r=i[0],h=r==="{"?"}":"]";let s=0,d=o;for(let n=o;n<t.length;n++)if(t[n]===r)s++;else if(t[n]===h&&(s--,s===0)){d=n+1;break}if(d>o)return t.substring(o,d)}else if(e==="XML"){const i=t.indexOf("<"),o=t.lastIndexOf(">");if(i!==-1&&o!==-1&&o>i)return t.substring(i,o+1)}return t}async fixWithGroq(a,e,t,i){var d,n,p;if(!t)throw new Error("Groq API key is required");const o=this.buildFixPrompt(a,e),r=await fetch("https://api.groq.com/openai/v1/chat/completions",{method:"POST",headers:{Authorization:`Bearer ${t}`,"Content-Type":"application/json"},body:JSON.stringify({model:i,messages:[{role:"system",content:`You are a ${e.type} syntax error fixing assistant. Only output valid ${e.type}, nothing else.`},{role:"user",content:o}],temperature:.1,max_tokens:16e3})});if(!r.ok){const l=await r.json().catch(()=>({error:{message:"Unknown error"}}));throw new Error(((d=l.error)==null?void 0:d.message)||`Groq API error: ${r.status}`)}let s=((p=(n=(await r.json()).choices[0])==null?void 0:n.message)==null?void 0:p.content)||"";if(s.includes("```")){const l=s.split(`
`),c=[];let m=!1;for(const f of l)f.trim().startsWith("```")?m=!m:m&&c.push(f);c.length>0?s=c.join(`
`):s=l.filter(f=>!f.trim().startsWith("```")).join(`
`)}return s=this.extractContent(s,e.type),s.trim()}async fixWithOpenAI(a,e,t,i){var d,n,p;if(!t)throw new Error("OpenAI API key is required");const o=this.buildFixPrompt(a,e),r=await fetch("https://api.openai.com/v1/chat/completions",{method:"POST",headers:{Authorization:`Bearer ${t}`,"Content-Type":"application/json"},body:JSON.stringify({model:i,messages:[{role:"system",content:`You are a ${e.type} syntax error fixing assistant. Only output valid ${e.type}, nothing else.`},{role:"user",content:o}],temperature:.1,max_tokens:16e3})});if(!r.ok){const l=await r.json().catch(()=>({error:{message:"Unknown error"}}));throw new Error(((d=l.error)==null?void 0:d.message)||`OpenAI API error: ${r.status}`)}let s=((p=(n=(await r.json()).choices[0])==null?void 0:n.message)==null?void 0:p.content)||"";if(s.includes("```")){const l=s.split(`
`),c=[];let m=!1;for(const f of l)f.trim().startsWith("```")?m=!m:m&&c.push(f);c.length>0?s=c.join(`
`):s=l.filter(f=>!f.trim().startsWith("```")).join(`
`)}return s=this.extractContent(s,e.type),s.trim()}async fixWithClaude(a,e,t,i){var d,n;if(!t)throw new Error("Claude API key is required");const o=this.buildFixPrompt(a,e),r=await fetch("https://api.anthropic.com/v1/messages",{method:"POST",headers:{"x-api-key":t,"anthropic-version":"2023-06-01","Content-Type":"application/json"},body:JSON.stringify({model:i,max_tokens:16e3,system:`You are a ${e.type} syntax error fixing assistant. Only output valid ${e.type}, nothing else.`,messages:[{role:"user",content:o}],temperature:.1})});if(!r.ok){const p=await r.json().catch(()=>({error:{message:"Unknown error"}}));throw new Error(((d=p.error)==null?void 0:d.message)||`Claude API error: ${r.status}`)}let s=((n=(await r.json()).content[0])==null?void 0:n.text)||"";if(s.includes("```")){const p=s.split(`
`),l=[];let c=!1;for(const m of p)m.trim().startsWith("```")?c=!c:c&&l.push(m);l.length>0?s=l.join(`
`):s=p.filter(m=>!m.trim().startsWith("```")).join(`
`)}return s=this.extractContent(s,e.type),s.trim()}async fix(a,e,t,i){const{provider:o,ollamaModel:r,groqApiKey:h,groqModel:s,openaiApiKey:d,openaiModel:n,claudeApiKey:p,claudeModel:l}=t;try{if(o==="ollama"){const c=r||"deepseek-r1:8b";return await this.fixWithOllama(a,e,c,i)}if(i&&i({progress:50,text:"Processing with AI...",timeElapsed:0}),o===w.GROQ)return await this.fixWithGroq(a,e,h,s);if(o===w.OPENAI)return await this.fixWithOpenAI(a,e,d,n);if(o===w.CLAUDE)return await this.fixWithClaude(a,e,p,l);throw new Error("Invalid AI provider")}catch(c){throw c}}async cleanup(){return Promise.resolve()}getSystemInfo(){return{platform:"desktop",aiProvider:"ollama + cloud",contextWindow:"8k-200k (model dependent)",requiresInternet:"Optional (Ollama is local)",privacyLevel:"Full privacy with Ollama",performance:"Native (fastest with Ollama)"}}}const k=new y;export{E as CLAUDE_MODELS,y as DesktopAIService,C as GROQ_MODELS,b as OLLAMA_MODELS,$ as OPENAI_MODELS,k as desktopAIService};
